### 机器学习基础知识

#### 基本术语

1. 数据集

2. 样本

3. 维度

   特征的数量

4. 特征

5. 假设空间

   定义了模型可以学习和表达的所有可能的假设

6. 归纳偏好

   算法对假设空间中的某些假设更有倾向性或偏好的一种倾向

#### 模型评估

1. 评估方法

   1. 留出法（常用）
   2. k倍交叉检验（用的最多）
   3. ~~自助法（不常用）~~ 

2. 性能度量

   > 回归任务常用均方误差 ==MSE== 

   **分类任务**

   1. 精度	acc

   2.  查全率 precision

      预测为真的中有多少为真

   3. 查准率 recall

      真的样例中有多少被预测为真

   4. F1

      查全率和查准率的调和平均

      调和均值赋予较小值更大的权重，并且调和均值对极端值敏感

   5. ROC

   6. AUC

#### 模型选择

**与数据是什么分布有关**

#### 线性回归

y = wx+b

多元线性回归：y = w1x+w2x+w3x+……+b

#### 对数几率回归（Logistic ）==分类==

使用sigmoid和Relu等函数，将线性模型的输出值映射到一个特定的

#### 基于线性模型的多分类

1. 1对1

   k(k-1)/2个分类器

   分别对没两个进行pk，统计最后胜出最多的那个

2. 1对多

   k个分类器

   每一个分类器仅能将其中一个同其他的进行区分

3. 多对多

### 机器学习方法

#### 感知机

![](http://image.zzzsleep.icu/20230730162033.png)

#### 反向传播算法

![img](http://image.zzzsleep.icu/2018050922060950)

#### 梯度下降算法

![img](http://image.zzzsleep.icu/v2-e1e6b238b5292251690526c055858fc6_b.jpg)

#### 深度学习

#### 支持向量机分类

> 关于支持向量和超平面确定的关系
>
> 他们两个是一起确定的
>
> ![image-20230730191937261](http://image.zzzsleep.icu/image-20230730191937261.png)

![img](http://image.zzzsleep.icu/v2-197913c461c1953c30b804b4a7eddfcc_b.webp)

#### 支持向量机回归

> 通过找到一个超平面，**使得所有数据点的预测值与实际值之间的差距都在一个预设的阈值内**，同时使得超平面尽可能的平坦（即其权重向量尽可能的小），来实现回归预测。

![img](http://image.zzzsleep.icu/v2-92143b4caffe51a2ddeaf336c37a0aeb_b.jpg)

#### 朴素贝叶斯分类器

> 基于概率的
>
> 并且假设每个特征都是独立的（朴素）

把拥有该特征是好瓜的概率连乘

拥有该特征是坏瓜的特征连乘

然后比较哪个概率比较大

#### 半朴素贝叶斯分类器

> 条件有依赖关系，没办法单独相乘了，要看成一起的
>
> 在朴素贝叶斯的基础上放宽了特征独立性的假设，允许一部分特征之间存在依赖关系，从而在保持计算简单性的同时提高了分类性能。

#### EM算法

> 如果给定的数据中存在缺失值，或者模型本身具有隐含的潜在变量，那么可以先==猜测==这些缺失或隐含的数据，然后基于这些猜测来估计模型参数，再基于新估计的参数去修改之前的猜测，如此反复迭代，直到模型参数收敛。
>
> **通过交替地进行期望（E）步骤和最大化（M）步骤，从而在存在隐变量的情况下估计参数，E步骤中计算隐变量的期望值，M步骤中最大化观测数据的似然函数。**

猜测的方法是根据数据的分布进行猜测

#### 集成学习

> 将弱学习器进行组合

#### Boosting 串行序列化

> 通过迭代地训练一系列的弱学习器，每个学习器都试图纠正前一个学习器的错误，然后将这些学习器的预测进行加权汇总，以提高模型的性能。

*集成学习中，Adaboost算法，每一轮训练后会增加原本被错误分类的样本的权重，这样做会不会导致，原本被正确分类的样本不会重新被分错？*

> 实际上，Adaboost算法通过增加被错误分类的样本权重，确实可能会影响到原本被正确分类的样本，可能会导致它们在后续的分类器中被错误地分类。这是一种权衡，我们愿意接受一些原来正确分类的样本被错误分类，以便更好地对那些被错误分类的样本进行分类。
>
> 然而，值得注意的是，Adaboost算法同时也在减小被正确分类的样本的权重，这就意味着，这些样本在后续的训练中的影响力会被降低。这样一来，虽然有可能会有一些原来正确分类的样本在后续的分类器中被错误地分类，但是这些错误的影响力是较小的，因为这些样本的权重已经被减小了。

#### Bagging和随机森林 并行

> Bagging 通过对原始数据集进行有放回抽样（即自助采样，Bootstrap Sampling）生成多个不同的数据子集，然后分别训练多个模型，最后对这些模型的预测结果进行投票或平均，得到最终的预测结果。
>
> 随机森林 选取不同的特征，训练不同的决策树，对决策进行整合

![image.png](http://image.zzzsleep.icu/1830)

#### Stacking算法

> 对多个学习器的结果进行学习最终进行预测
>
> 通过训练多个不同的基模型，然后使用一个元模型（或称为二级模型）来学习如何最好地组合这些基模型的预测，从而实现更精确的预测。

#### k-means算法（k均值）

> 将数据集分为 K 个不同的子集（或聚类），使得同一个子集内的数据点尽可能相似（即距离尽可能近），不同子集之间的数据点尽可能不相似（即距离尽可能远）
>
> 通过迭代地分配数据点到最近的簇中心，并更新簇中心为簇内数据点的均值，从而将数据划分为k个聚类。
>
> ==缺点：需要预先确定k的值==

![img](http://image.zzzsleep.icu/20180712180110142)

#### 学习向量化（LVQ）

> 一般用于对于已经做好分类的，再次进行分类（缩小最终得到的分类数量）

#### 高斯混合聚类（GMM）

> ==假设数据是由若干高斯分布混合生成的==
>
> 并通过最大化数据的似然，学习每个高斯分布的参数，从而将每个数据点分配给最可能生成它的高斯分布，实现聚类。
>
> 随机选择K个点做为初始的数据点
>
> 然后使用EM算法进行跟更新
>
> **疑问？EM算法正确使用的前提是不是数据分布预测正确**

![See the source image](http://image.zzzsleep.icu/0c7b5134a05df8ea8071f5a3f09b67cf.png)



#### 密度聚类

> 通过在数据空间中寻找连续的高密度区域并将其划分为聚类，同时将低密度区域视为噪声或者聚类之间的边界。
>
> 核心对象：在给定的参数 Eps（邻域半径）和 MinPts（最小点数）下，如果在一个对象的 Eps 邻域内的对象数目大于等于 MinPts，那么这个对象就是一个核心对象
>
> 密度直达：对于数据集中的两个点 A 和 B，如果 B 在 A 的 Eps 邻域内，并且 A 是一个核心对象，那么我们就说 B 是从 A 密度直达的。
>
>  密度可达：
>
> 密度相连：对于数据集中的两个点 A 和 B，如果存在一个点 O，使得 A 和 B 都是从 O 密度可达的，那么我们就说 A 和 B 是密度相连的

![img](http://image.zzzsleep.icu/DensityClustering.png)

![img](http://image.zzzsleep.icu/1042406-20161222112847323-1346197243.png)

#### 层次聚类

> 反复合并直到停止条件

![img](http://image.zzzsleep.icu/v2-3aed2646f89280472646264b8a740242_b.jpg)

#### k近邻学习

> 对于一个未标记的数据点，参考其最近的 K 个邻居的标签（在分类中通常是通过投票，在回归中通常是通过平均），并通过这些邻居的标签来预测该数据点的标签。

![img](http://image.zzzsleep.icu/648116-20190704002300363-936669533.png)

![img](http://image.zzzsleep.icu/648116-20190704002731502-540487085.png)

![img](http://image.zzzsleep.icu/648116-20190704002827655-1839629497.png)

#### 低维嵌入

> 低维嵌入可以被概括为：通过寻找一个低维空间并将高维数据映射到这个空间，以保留数据的某些重要特性（如距离或拓扑结构），从而简化数据的复杂性和维度。

![39x1](http://image.zzzsleep.icu/202307301928809.png)

#### 核化线性降维

> 核化线性降维可以被概括为：首先通过将数据映射到一个更高维的特征空间，然后在这个高维空间中应用线性降维技术，以达到在原始空间中解决非线性问题的目的。



#### 主成分分析

> 主成分分析（PCA）可以被概括为：通过找到数据的主要变异方向并将数据投影到这些方向上，从而减少数据的维度并保留数据的主要特性。

#### 流形学习

> 通过揭示并利用高维数据中的低维几何结构（即流形结构），以更好地理解和表示数据，从而实现数据的降维和特征提取。



#### numpy

1. **多维数组**：Numpy 的核心特性是 N 维数组对象（ndarray），这使得它在处理大量数值数据时非常高效。在机器学习中，我们常常需要处理大量的数据集，这些数据通常被表示为矩阵或高维数组，比如特征矩阵（每行是一个样本，每列是一个特征）。
2. **数学运算**：Numpy 提供了大量的数学函数，包括但不限于统计函数、线性代数函数、傅里叶转换等。这些都是实施机器学习算法所必需的。
3. **随机数生成**：在机器学习中，随机数生成是非常重要的一个环节，比如在初始化神经网络的权重，或者在执行某些随机化算法（例如，随机梯度下降）时，Numpy 提供了各类随机数生成的函数。
4. **广播功能**：Numpy 的广播功能使得不同大小的数组之间可以进行数学计算，这对于数据预处理和特征工程来说非常有用。
5. **与其他库的整合**：**Numpy 与许多 Python 的其他科学计算库（如 Pandas、Matplotlib、Scikit-learn等）都有良好的集成，这些库往往在内部使用 Numpy 数组进行计算。** 

#### pandas

1. **数据导入和导出**：Pandas 提供了各种工具，**可以帮助我们导入和导出各种格式的数据**，包括 CSV、Excel、SQL 数据库和 HDF5 格式。
2. **数据清洗**：Pandas 提供了许多用于数据清洗的工具，例如删除空值、填充空值、删除重复的行、更改列名等。
3. **数据探索和分析**：Pandas 提供了各种数据探索和分析的工具，例如计算描述性统计量、查找唯一值、计算列之间的相关性等。
4. **数据预处理**：Pandas 提供了各种数据预处理的工具，例如对数据进行排序、过滤、分组、合并、reshape等。
5. **特征工程**：在机器学习中，特征工程是一个非常重要的步骤，Pandas 可以帮助我们创建和修改特征。例如，我们可以用 Pandas 来创建新的特征，或者将分类特征转换为数值特征。
6. **数据可视化**：虽然 Pandas 不是一个专门的可视化库，但它提供了一些基于 Matplotlib 的绘图功能，可以帮助我们更好地理解数据。

#### scipy

1. **优化算法**：SciPy 提供了大量的优化算法，如**最小二乘法**、**梯度下降**、**牛顿法**等，这些都是许多机器学习算法的基础。
2. **统计函数**：SciPy 包含了丰富的统计函数，**包括描述统计、概率分布、统计检验等**，这些函数可以用来了解==数据的分布和关系==，也可以用来评估和改进模型。
3. **数值积分**：SciPy 提供了数值积分的功能，这在一些涉及到概率密度函数的机器学习算法中可能会用到。
4. **信号处理**：如果你的机器学习任务涉及到信号处理（例如声音、图像、时间序列等），SciPy 提供了大量的信号处理函数。
5. **线性代数**：虽然 NumPy 也提供了一些线性代数的功能，但 SciPy 提供了更全面和高级的线性代数函数，如==矩阵分解、矩阵函数==、特殊矩阵等。
6. **稀疏矩阵**：在一些大规模的机器学习任务中，数据可能会以稀疏矩阵的形式出现，SciPy 提供了大量的稀疏矩阵存储格式以及相关的数学运算。
7. **空间数据结构和算法**：例如，k-d 树，Delaunay 三角剖分和凸包等，这些在某些机器学习算法中非常有用，如 k-最近邻，主成分分析等。
8. **图像处理**：SciPy 提供了大量的图像处理功能，包括滤波、分割、形态学操作等，这对于图像相关的机器学习任务非常有用。

#### matplotlib

> 模型可视化和tensorboard重合，检验用tensorboard

#### sklearn

<img src="http://image.zzzsleep.icu/202307301943600.png" alt="img" style="zoom:150%;" />

<img src="http://image.zzzsleep.icu/202307301946210.png" alt="img" style="zoom:150%;" />
