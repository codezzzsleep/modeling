线性回归（Linear Regression）是一种简单而又实用的机器学习模型，主要用于预测一个因变量（目标变量）与一个或多个自变量（特征）之间的线性关系。线性回归经常用于分析变量之间的关系，例如根据房屋的面积来预测房价。

线性回归模型可以分为以下两种：

1. 一元线性回归（Simple Linear Regression）：只有一个自变量和一个因变量。模型表示为：y = a + bx + ε，其中y是因变量（目标），x是自变量（特征），a是截距项，b是斜率，ε代表误差。

2. 多元线性回归（Multiple Linear Regression）：包含多个自变量。模型表示为：y = a + b1x1 + b2x2 + ... + bnxn + ε，其中y是因变量，x1, x2, ..., xn是自变量，a是截距项，b1, b2, ..., bn是回归系数，ε代表误差。

在训练线性回归模型时，我们的目标是找到回归系数，使得模型预测的结果与实际值之间的误差最小。通常使用均方误差（Mean Squared Error, MSE）作为损失函数，求解可以通过最小二乘法（Ordinary Least Squares, OLS）或者梯度下降（Gradient Descent）等优化算法来实现。

线性回归模型的优点是简单、易于理解和实现。然而，在非线性关系中，线性回归模型的表现就相对较差。在这种情况下，可以考虑使用多项式回归（Polynomial Regression）或其他非线性模型进行预测。